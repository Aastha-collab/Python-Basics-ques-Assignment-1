{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Learning - Assignment**"
      ],
      "metadata": {
        "id": "HEfINOgAHHwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.**"
      ],
      "metadata": {
        "id": "sdvJXkifHTV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble learning is a machine learning technique that combines multiple models to produce a more accurate and robust prediction than any single model alone.\n",
        "\n",
        "The key idea is that by combining diverse \"weak learners,\" their individual errors cancel out, leading to improved overall performance, reduced bias, and greater reliability."
      ],
      "metadata": {
        "id": "HDEaWcvbHhGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Bagging and Boosting?**"
      ],
      "metadata": {
        "id": "5f-WC4SRHqVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging builds multiple models in parallel on different subsets of data, while boosting builds models sequentially, with each new model trying to correct the errors of the previous one.\n",
        "\n",
        "Bagging reduces variance by averaging independent models, whereas boosting reduces bias by giving more weight to misclassified data points."
      ],
      "metadata": {
        "id": "PiUoHF8HHtkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**"
      ],
      "metadata": {
        "id": "0FomByk6H3Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bootstrap sampling is a resampling technique where multiple datasets are created from a single dataset by randomly sampling with replacement.\n",
        "\n",
        "This process is crucial for Bagging (Bootstrap Aggregating) methods like Random Forest because it creates diverse training sets for each model (e.g., decision tree), which helps to reduce variance and prevent overfitting by decorrelating the individual models."
      ],
      "metadata": {
        "id": "9Ara6RFNH8B9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**"
      ],
      "metadata": {
        "id": "UUCfhftlIEiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out-of-Bag (OOB) samples are data points from the original dataset that are not included in a specific bootstrap sample used to train one tree in an ensemble model like a Random Forest.\n",
        "\n",
        " The OOB score is an evaluation metric calculated by using these OOB samples to estimate the model's performance, acting as a built-in validation set."
      ],
      "metadata": {
        "id": "xU340vEYIJp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n"
      ],
      "metadata": {
        "id": "JbANlAv4IX4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stability and Robustness**: A single Decision Tree's feature importance can be highly unstable and sensitive to small changes in the training data, leading to fluctuating importance scores. Random Forests, by averaging feature importances across multiple trees trained on bootstrapped samples, provide more stable and robust estimates less prone to noise or minor data variations.\n",
        "\n",
        "**Overfitting Bias:** A single Decision Tree is prone to overfitting, which can lead to inflated importance scores for features that might be highly relevant only to the specific training data. Random Forests mitigate this by considering feature importance across diverse trees, reducing the bias towards features that might be overfit in a single tree."
      ],
      "metadata": {
        "id": "6GuTm3L8IcF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "- Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "- Train a Random Forest Classifier\n",
        "- Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "v_3sC1GNI4RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "# [1]\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "# [2]\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a pandas Series to associate feature names with their importance scores\n",
        "feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "# [3]\n",
        "print(\"Top 5 most important features:\")\n",
        "print(feature_importances.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJyeJwjOJFRO",
        "outputId": "38d08528-4aaa-4aa0-dc7e-615d87691f70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "- Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "- Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "jwyvX_3DJR6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Train and evaluate a single Decision Tree Classifier\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "y_pred_single = single_tree.predict(X_test)\n",
        "accuracy_single_tree = accuracy_score(y_test, y_pred_single)\n",
        "print(f\"Accuracy of a single Decision Tree: {accuracy_single_tree:.4f}\")\n",
        "\n",
        "# 2. Train and evaluate a Bagging Classifier with Decision Trees as base estimators\n",
        "# n_estimators: number of base estimators (Decision Trees) in the ensemble\n",
        "# base_estimator: the estimator to be used as the base learner (DecisionTreeClassifier)\n",
        "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42),\n",
        "                                       n_estimators=10,\n",
        "                                       random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_classifier.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "print(f\"Accuracy of Bagging Classifier: {accuracy_bagging:.4f}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "if accuracy_bagging > accuracy_single_tree:\n",
        "    print(\"\\nThe Bagging Classifier performed better than the single Decision Tree.\")\n",
        "elif accuracy_bagging < accuracy_single_tree:\n",
        "    print(\"\\nThe single Decision Tree performed better than the Bagging Classifier.\")\n",
        "else:\n",
        "    print(\"\\nThe Bagging Classifier and the single Decision Tree achieved the same accuracy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px4qLdI5J2HF",
        "outputId": "e2bf1326-ca99-4f5d-cc40-a99e1492b66a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of a single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier: 1.0000\n",
            "\n",
            "The Bagging Classifier and the single Decision Tree achieved the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "- Train a Random Forest Classifier\n",
        "- Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "- Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "JPjeK0zQKBIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris # Using a sample dataset for demonstration\n",
        "\n",
        "# Load a sample dataset (Iris dataset)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20]      # Maximum depth of the tree\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Perform GridSearchCV to find the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best estimator (model with the best parameters)\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the final accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Final Accuracy with best parameters:\", final_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFLerWyTKJDM",
        "outputId": "e77b1963-1947-4204-b75b-2f0e42253378"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy with best parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "- Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "- Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "WqegHjWsKQ2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california_housing = fetch_california_housing(as_frame=True)\n",
        "X = california_housing.data\n",
        "y = california_housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Bagging Regressor\n",
        "bagging_regressor = BaggingRegressor(random_state=42)\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the Bagging Regressor\n",
        "y_pred_bagging = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error for Bagging Regressor\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "# Initialize and train the Random Forest Regressor\n",
        "random_forest_regressor = RandomForestRegressor(random_state=42)\n",
        "random_forest_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the Random Forest Regressor\n",
        "y_pred_random_forest = random_forest_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error for Random Forest Regressor\n",
        "mse_random_forest = mean_squared_error(y_test, y_pred_random_forest)\n",
        "\n",
        "# Print the Mean Squared Errors\n",
        "print(f\"Mean Squared Error (Bagging Regressor): {mse_bagging:.4f}\")\n",
        "print(f\"Mean Squared Error (Random Forest Regressor): {mse_random_forest:.4f}\")\n",
        "\n",
        "# Compare and determine the better model\n",
        "if mse_bagging < mse_random_forest:\n",
        "    print(\"The Bagging Regressor performed better (lower MSE).\")\n",
        "elif mse_random_forest < mse_bagging:\n",
        "    print(\"The Random Forest Regressor performed better (lower MSE).\")\n",
        "else:\n",
        "    print(\"Both regressors achieved the same MSE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk9PeYJUKaPA",
        "outputId": "dd31d938-203d-4fa2-d3f8-0369619e63b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.2824\n",
            "Mean Squared Error (Random Forest Regressor): 0.2554\n",
            "The Random Forest Regressor performed better (lower MSE).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "- Choose between Bagging or Boosting\n",
        "- Handle overfitting\n",
        "- Select base models\n",
        "- Evaluate performance using cross-validation\n",
        "- Justify how ensemble learning improves decision-making in this real-world context."
      ],
      "metadata": {
        "id": "TUbCUi5lKj7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To predict loan defaults, choose Bagging for reducing variance (e.g., Random Forest) or Boosting for reducing bias (e.g., Gradient Boosting) based on initial model performance.\n",
        "\n",
        " Prevent overfitting by using techniques like regularization, pruning, or early stopping, and by employing cross-validation to get an unbiased performance estimate.\n",
        "\n",
        " Select base models like decision trees or logistic regression, which are robust and interpretable for this task, then combine them using ensemble methods to improve accuracy and decision-making, as ensemble models handle diverse data patterns better than single models."
      ],
      "metadata": {
        "id": "01FPWToLKp__"
      }
    }
  ]
}