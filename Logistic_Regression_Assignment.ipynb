{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression - Assignment**"
      ],
      "metadata": {
        "id": "10DrkTVbReX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Logistic Regression, and how does it differ from Linear Regression?**"
      ],
      "metadata": {
        "id": "p9IvZZzIRhAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression predicts the probability of a categorical outcome (like \"yes\" or \"no\"), while linear regression predicts a continuous value (like a house price).\n",
        "\n",
        "The main differences are that logistic regression uses a sigmoid (S-shaped) curve for binary classification and outputs a probability between 0 and 1, whereas linear regression uses a straight line and can output any number.  "
      ],
      "metadata": {
        "id": "O2K7-UPdRqp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the role of the Sigmoid function in Logistic Regression.**"
      ],
      "metadata": {
        "id": "m8ki8JH8R2oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sigmoid function is a crucial part of logistic regression, used to map a linear combination of inputs to a probability between 0 and 1."
      ],
      "metadata": {
        "id": "31YhjBZIR7R5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is Regularization in Logistic Regression and why is it needed?**"
      ],
      "metadata": {
        "id": "vimrDYa5SiUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization in logistic regression is a technique that penalizes model complexity to prevent overfitting and improve the model's ability to generalize to new, unseen data.\n",
        "\n",
        "This is achieved by adding a penalty term to the model's loss function, which discourages the model from assigning excessively large weights (coefficients) to features."
      ],
      "metadata": {
        "id": "MinLsBIvSnjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are some common evaluation metrics for classification models, and why are they important?**"
      ],
      "metadata": {
        "id": "Pn2N_950S6xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common classification metrics include accuracy, precision, recall, the F1-score, and AUC-ROC, which measure how well a model distinguishes between classes.\n",
        "\n",
        "These metrics are crucial for evaluating a model's performance, understanding its strengths and weaknesses (like its ability to handle imbalanced datasets), and selecting the best model for a specific task"
      ],
      "metadata": {
        "id": "j2Zgd9KCTFGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy. (Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "oaAh68h-sf7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# 1. Load a dataset (using a built-in sklearn dataset for demonstration)\n",
        "# In a real-world scenario, you would use pd.read_csv('your_file.csv')\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "# test_size=0.3 means 30% of the data will be used for testing\n",
        "# random_state ensures reproducibility of the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200) # max_iter increased for convergence with some datasets\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Logistic Regression model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiburnbssnxE",
        "outputId": "6cf557e9-6f3c-43ad-f4fd-ae9d46c11ddb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Logistic Regression model: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy. (Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "rCiTSnASs0QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a dataset (e.g., Iris dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train a Logistic Regression model with L2 regularization\n",
        "# The 'penalty' parameter is set to 'l2' for L2 regularization (Ridge)\n",
        "# The 'C' parameter controls the inverse of regularization strength; smaller C means stronger regularization.\n",
        "# 'solver' is chosen for its compatibility with 'l2' penalty and multi-class classification.\n",
        "model = LogisticRegression(penalty='l2', C=0.1, solver='lbfgs', multi_class='auto', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n",
        "print(\"\\nModel Intercept:\")\n",
        "print(model.intercept_)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX0xKvTxs8dN",
        "outputId": "364c9322-209b-4949-99ed-9763cb7a59e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "[[-0.26735189  0.29439384 -1.02390677 -0.41212167]\n",
            " [ 0.02959517 -0.3373663   0.07552206 -0.15957599]\n",
            " [ 0.23775672  0.04297246  0.94838471  0.57169765]]\n",
            "\n",
            "Model Intercept:\n",
            "[ 4.54765055  1.6115804  -6.15923095]\n",
            "\n",
            "Model Accuracy: 0.9556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report. (Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "hYjHdWuFtE3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Suppress ConvergenceWarning for demonstration purposes\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# 1. Load a multiclass dataset from scikit-learn\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Instantiate and train the Logistic Regression model with multi_class='ovr'\n",
        "# The 'liblinear' solver is efficient for smaller datasets and handles 'ovr' well.\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1tlpsQvtKV6",
        "outputId": "5a07dce1-14dd-4aca-bdda-498c4bc015c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        19\n",
            "  versicolor       1.00      0.92      0.96        13\n",
            "   virginica       0.93      1.00      0.96        13\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy. (Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "ADxYpQL0tTr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features (important for Logistic Regression with regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
        "    'penalty': ['l1', 'l2']  # Regularization type (l1 or l2)\n",
        "}\n",
        "\n",
        "log_reg = LogisticRegression(solver='liblinear', max_iter=200)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Print the best validation accuracy\n",
        "print(\"\\nBest validation accuracy (mean cross-validation score):\")\n",
        "print(f\"{grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate the best estimator on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test_scaled, y_test)\n",
        "print(f\"\\nTest set accuracy with best parameters: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtVnKsJQtmIk",
        "outputId": "364052b1-0eac-48a9-c43d-f1993ab84ada"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV:\n",
            "{'C': 10, 'penalty': 'l1'}\n",
            "\n",
            "Best validation accuracy (mean cross-validation score):\n",
            "0.9583\n",
            "\n",
            "Test set accuracy with best parameters: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling. (Use Dataset from sklearn package)**\n"
      ],
      "metadata": {
        "id": "v5aChzFHtuqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Model without scaling ---\n",
        "print(\"--- Model without scaling ---\")\n",
        "# Initialize and train Logistic Regression model\n",
        "model_no_scaling = LogisticRegression(max_iter=200, random_state=42)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "\n",
        "# --- Model with scaling ---\n",
        "print(\"\\n--- Model with scaling ---\")\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit scaler on training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train Logistic Regression model on scaled data\n",
        "model_scaled = LogisticRegression(max_iter=200, random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy on scaled data\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roxfh8GXt0Dk",
        "outputId": "b6b1d63f-70a9-47d7-bd6e-8974deba3e63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model without scaling ---\n",
            "Accuracy without scaling: 1.0000\n",
            "\n",
            "--- Model with scaling ---\n",
            "Accuracy with scaling: 1.0000\n",
            "\n",
            "Comparison:\n",
            "Accuracy without scaling: 1.0000\n",
            "Accuracy with scaling: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.**"
      ],
      "metadata": {
        "id": "siYMgBYDt6vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When building a logistic regression model for an imbalanced e-commerce marketing campaign, the approach would involve several key steps.\n",
        "\n",
        "First, during data handling and preprocessing, standardize numerical features to prevent high-magnitude features from dominating the model. Then, address the class imbalance, where only 5% of customers respond, using a technique like oversampling the minority class (respondents) with SMOTE or using class weights within the logistic regression algorithm itself. This prevents the model from becoming biased toward the majority class (non-responders), which would result in poor performance in predicting who will actually respond.\n",
        "\n",
        "For hyperparameter tuning, use cross-validation, such as Stratified K-Fold, to ensure each fold maintains the same class distribution as the original dataset. Tune key hyperparameters like the regularization strength (C) and the algorithm's class weight settings to optimize for the business objective.\n",
        "\n",
        "Finally, evaluate the model not by accuracy, which is misleading with imbalanced data, but with relevant business metrics such as Precision, Recall, F1-score, and AUC-ROC, which provide a more comprehensive view of the model's ability to correctly identify potential responders."
      ],
      "metadata": {
        "id": "qAuCAM6uuJpr"
      }
    }
  ]
}