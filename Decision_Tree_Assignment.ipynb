{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree - Assignment**"
      ],
      "metadata": {
        "id": "DboofJlMOeWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Decision Tree, and how does it work in the context of classification?**\n"
      ],
      "metadata": {
        "id": "ALYYi6S9Oh6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree is a supervised machine learning algorithm that uses a flowchart-like, tree-structured model to make predictions.\n",
        "\n",
        "In classification, it uses an iterative \"divide and conquer\" approach to split a dataset into increasingly homogeneous subsets, assigning a class label to each final subset."
      ],
      "metadata": {
        "id": "XY2n-h2-OnEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**"
      ],
      "metadata": {
        "id": "U9TTHi98Ozm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini Impurity and Entropy are impurity measures used to determine the best split in a decision tree node.\n",
        "\n",
        "Gini Impurity measures the probability of a random data point being misclassified, while Entropy measures the disorder or randomness in the data. Both criteria aim to minimize impurity, with the tree selecting the split that results in the greatest reduction in Gini Impurity or Entropy, leading to more pure (less mixed) child nodes."
      ],
      "metadata": {
        "id": "B7DcpL1GiK5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**"
      ],
      "metadata": {
        "id": "i7bVijhkicZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-pruning stops a decision tree from growing by applying constraints during construction, while post-pruning grows a full tree and then removes branches afterward.\n",
        "\n",
        "A practical advantage of pre-pruning is its efficiency in computational resources, as it avoids the cost of building a large tree that is later pruned.\n",
        "\n",
        "A practical advantage of post-pruning is that it can lead to more accurate results than pre-pruning by considering all possible splits, even those that appear unhelpful early on, before making a final decision."
      ],
      "metadata": {
        "id": "aUIVHs8_ihbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**"
      ],
      "metadata": {
        "id": "uPtWC7uUi9Pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Gain (IG) is a measure used in decision trees to select the best feature for splitting data by quantifying the reduction in entropy (or impurity) after the split.\n",
        "\n",
        "It is important because the feature that results in the highest information gain is chosen at each step, as this feature is the most effective at separating the data into more homogenous subsets, leading to a more accurate and efficient tree."
      ],
      "metadata": {
        "id": "d-jG5jnsjApf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**"
      ],
      "metadata": {
        "id": "puNEym2ajTto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common applications of decision trees include medical diagnosis, fraud detection, customer segmentation, and loan approval.\n",
        "\n",
        "Key advantages are their ease of interpretation, minimal data preparation requirements, and ability to handle both numerical and categorical data.\n",
        "\n",
        "However, they are prone to overfitting, are unstable to small data changes, and can become computationally expensive and complex for large datasets"
      ],
      "metadata": {
        "id": "mPm0HmR_jaP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info:\n",
        "\n",
        "- Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "- Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "TpujMVuGjitw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "4KXyAKhcj43s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "AXksgv4tj_Cp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model’s accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(feature_names, dt_classifier.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM7kYzN_kDRy",
        "outputId": "286511fe-1f2f-4150-989a-6f50dede1ba7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
      ],
      "metadata": {
        "id": "pkCaPnT-kSqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Classifier with max_depth=3\n",
        "dt_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_pruned.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for the pruned tree\n",
        "y_pred_pruned = dt_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_pruned:.4f}\")\n",
        "\n",
        "# 3. Train a fully-grown Decision Tree Classifier (no max_depth limit)\n",
        "dt_full = DecisionTreeClassifier(random_state=42) # No max_depth specified means fully grown\n",
        "dt_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for the fully-grown tree\n",
        "y_pred_full = dt_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of fully-grown Decision Tree: {accuracy_full:.4f}\")\n",
        "\n",
        "# Compare accuracies\n",
        "if accuracy_pruned > accuracy_full:\n",
        "    print(\"\\nDecision Tree with max_depth=3 performed better.\")\n",
        "elif accuracy_full > accuracy_pruned:\n",
        "    print(\"\\nFully-grown Decision Tree performed better.\")\n",
        "else:\n",
        "    print(\"\\nBoth Decision Trees performed equally well.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xLqQ568kqeT",
        "outputId": "d5c4513b-1d99-4323-a8e6-6ffa441fd800"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with max_depth=3: 1.0000\n",
            "Accuracy of fully-grown Decision Tree: 1.0000\n",
            "\n",
            "Both Decision Trees performed equally well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "- Load the Boston Housing Dataset\n",
        "- Train a Decision Tree Regressor\n",
        "- Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "i42LOTDDkzJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing Dataset (assuming a CSV named 'boston_housing.csv' is available)\n",
        "try:\n",
        "    df = pd.read_csv('boston_housing.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"boston_housing.csv not found. Creating a dummy dataset for demonstration.\")\n",
        "    # Create a dummy dataset resembling the Boston Housing structure for demonstration\n",
        "    data = {\n",
        "        'CRIM': [0.00632, 0.02731, 0.02729, 0.03237, 0.06905],\n",
        "        'ZN': [18.0, 0.0, 0.0, 0.0, 0.0],\n",
        "        'INDUS': [2.31, 7.07, 7.07, 2.18, 2.18],\n",
        "        'CHAS': [0, 0, 0, 0, 0],\n",
        "        'NOX': [0.538, 0.469, 0.469, 0.458, 0.458],\n",
        "        'RM': [6.575, 6.421, 7.185, 6.998, 7.147],\n",
        "        'AGE': [65.2, 78.9, 61.1, 45.8, 54.2],\n",
        "        'DIS': [4.0900, 4.9671, 4.9671, 6.0622, 6.0622],\n",
        "        'RAD': [1, 2, 2, 3, 3],\n",
        "        'TAX': [296, 242, 242, 222, 222],\n",
        "        'PTRATIO': [15.3, 17.8, 17.8, 18.7, 18.7],\n",
        "        'B': [396.90, 396.90, 392.83, 394.63, 396.90],\n",
        "        'LSTAT': [4.98, 9.14, 4.03, 2.94, 5.33],\n",
        "        'MEDV': [24.0, 21.6, 34.7, 33.4, 36.2]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('MEDV', axis=1)  # 'MEDV' is the target variable (median home value)\n",
        "y = df['MEDV']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train a Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = pd.Series(dt_regressor.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ3lS9GzlJ4k",
        "outputId": "5588fa08-3d2d-44de-c166-76862c426204"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boston_housing.csv not found. Creating a dummy dataset for demonstration.\n",
            "Mean Squared Error (MSE): 5.76\n",
            "\n",
            "Feature Importances:\n",
            "AGE        0.956787\n",
            "LSTAT      0.033914\n",
            "DIS        0.009299\n",
            "ZN         0.000000\n",
            "CRIM       0.000000\n",
            "NOX        0.000000\n",
            "CHAS       0.000000\n",
            "INDUS      0.000000\n",
            "RM         0.000000\n",
            "RAD        0.000000\n",
            "TAX        0.000000\n",
            "PTRATIO    0.000000\n",
            "B          0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "- Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "3cViMwRWlbK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Tune the Decision Tree max_depth and min_samples_split using GridSearchCV\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 7, 10],\n",
        "    'min_samples_split': [2, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Perform the grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 3. Print the best parameters and the resulting model accuracy\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best estimator (model)\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_pred = best_dt_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the best model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXcskfZ5lh0F",
        "outputId": "fb232653-1e4f-4452-c545-f6cdccffef63"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'max_depth': None, 'min_samples_split': 10}\n",
            "Accuracy of the best model: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "- Handle the missing values\n",
        "- Encode the categorical features\n",
        "- Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world setting."
      ],
      "metadata": {
        "id": "tLVRLcTGlo54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Handle the missing values: Impute missing numerical values using the mean, median, or mode, and missing categorical values with a new category or the mode.\n",
        "\n",
        "- Encode the categorical features: Use one-hot encoding for nominal categories and label encoding for ordinal categories to convert them into a numerical format the model can process.\n",
        "\n",
        "- Train a Decision Tree model: Split the dataset into training and testing sets and fit the Decision Tree model to the training data.\n",
        "\n",
        "- Tune its hyperparameters: Use techniques like Grid Search or Randomized Search with cross-validation to find the optimal values for parameters such as max_depth and min_samples_leaf.\n",
        "\n",
        "- Evaluate its performance: Assess the model using metrics like accuracy, precision, recall, F1-score, and the confusion matrix on the testing set to understand its effectiveness.\n",
        "\n",
        "- Business value: The model can assist clinicians with early, data-driven disease prediction, leading to earlier intervention, improved patient outcomes, and potentially reduced healthcare costs."
      ],
      "metadata": {
        "id": "_4ZGhsVJl2nH"
      }
    }
  ]
}